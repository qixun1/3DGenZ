# -----------------------------------#
# Parameters of the training session #
# -----------------------------------#

# Input parameters
# ****************

dataset = SemanticKitti
dataset_task = slam_segmentation
num_classes = 20
in_points_dim = 3
in_features_dim = 2
in_radius = 4.000000
input_threads = 10

# Model parameters
# ****************

architecture = simple resnetb resnetb_strided resnetb resnetb resnetb_strided resnetb resnetb resnetb_strided resnetb resnetb resnetb_strided resnetb nearest_upsample unary nearest_upsample unary nearest_upsample unary nearest_upsample unary
equivar_mode = 
invar_mode = 
num_layers = 5
first_features_dim = 128
use_batch_norm = 1
batch_norm_momentum = 0.020000

segmentation_ratio = 1.000000

# KPConv parameters
# *****************

first_subsampling_dl = 0.060000
num_kernel_points = 15
conv_radius = 2.500000
deform_radius = 6.000000
fixed_kernel_points = center
KP_extent = 1.200000
KP_influence = linear
aggregation_mode = sum
modulated = 0
n_frames = 1
max_in_points = 5768

max_val_points = 7823

val_radius = 4.000000

# Training parameters
# *******************

learning_rate = 0.010000
momentum = 0.980000
lr_decay_epochs = 1:0.984767 2:0.984767 3:0.984767 4:0.984767 5:0.984767 6:0.984767 7:0.984767 8:0.984767 9:0.984767 10:0.984767 11:0.984767 12:0.984767 13:0.984767 14:0.984767 15:0.984767 16:0.984767 17:0.984767 18:0.984767 19:0.984767 20:0.984767 21:0.984767 22:0.984767 23:0.984767 24:0.984767 25:0.984767 26:0.984767 27:0.984767 28:0.984767 29:0.984767 30:0.984767 31:0.984767 32:0.984767 33:0.984767 34:0.984767 35:0.984767 36:0.984767 37:0.984767 38:0.984767 39:0.984767 40:0.984767 41:0.984767 42:0.984767 43:0.984767 44:0.984767 45:0.984767 46:0.984767 47:0.984767 48:0.984767 49:0.984767 50:0.984767 51:0.984767 52:0.984767 53:0.984767 54:0.984767 55:0.984767 56:0.984767 57:0.984767 58:0.984767 59:0.984767 60:0.984767 61:0.984767 62:0.984767 63:0.984767 64:0.984767 65:0.984767 66:0.984767 67:0.984767 68:0.984767 69:0.984767 70:0.984767 71:0.984767 72:0.984767 73:0.984767 74:0.984767 75:0.984767 76:0.984767 77:0.984767 78:0.984767 79:0.984767 80:0.984767 81:0.984767 82:0.984767 83:0.984767 84:0.984767 85:0.984767 86:0.984767 87:0.984767 88:0.984767 89:0.984767 90:0.984767 91:0.984767 92:0.984767 93:0.984767 94:0.984767 95:0.984767 96:0.984767 97:0.984767 98:0.984767 99:0.984767 100:0.984767 101:0.984767 102:0.984767 103:0.984767 104:0.984767 105:0.984767 106:0.984767 107:0.984767 108:0.984767 109:0.984767 110:0.984767 111:0.984767 112:0.984767 113:0.984767 114:0.984767 115:0.984767 116:0.984767 117:0.984767 118:0.984767 119:0.984767 120:0.984767 121:0.984767 122:0.984767 123:0.984767 124:0.984767 125:0.984767 126:0.984767 127:0.984767 128:0.984767 129:0.984767 130:0.984767 131:0.984767 132:0.984767 133:0.984767 134:0.984767 135:0.984767 136:0.984767 137:0.984767 138:0.984767 139:0.984767 140:0.984767 141:0.984767 142:0.984767 143:0.984767 144:0.984767 145:0.984767 146:0.984767 147:0.984767 148:0.984767 149:0.984767 150:0.984767 151:0.984767 152:0.984767 153:0.984767 154:0.984767 155:0.984767 156:0.984767 157:0.984767 158:0.984767 159:0.984767 160:0.984767 161:0.984767 162:0.984767 163:0.984767 164:0.984767 165:0.984767 166:0.984767 167:0.984767 168:0.984767 169:0.984767 170:0.984767 171:0.984767 172:0.984767 173:0.984767 174:0.984767 175:0.984767 176:0.984767 177:0.984767 178:0.984767 179:0.984767 180:0.984767 181:0.984767 182:0.984767 183:0.984767 184:0.984767 185:0.984767 186:0.984767 187:0.984767 188:0.984767 189:0.984767 190:0.984767 191:0.984767 192:0.984767 193:0.984767 194:0.984767 195:0.984767 196:0.984767 197:0.984767 198:0.984767 199:0.984767 200:0.984767 201:0.984767 202:0.984767 203:0.984767 204:0.984767 205:0.984767 206:0.984767 207:0.984767 208:0.984767 209:0.984767 210:0.984767 211:0.984767 212:0.984767 213:0.984767 214:0.984767 215:0.984767 216:0.984767 217:0.984767 218:0.984767 219:0.984767 220:0.984767 221:0.984767 222:0.984767 223:0.984767 224:0.984767 225:0.984767 226:0.984767 227:0.984767 228:0.984767 229:0.984767 230:0.984767 231:0.984767 232:0.984767 233:0.984767 234:0.984767 235:0.984767 236:0.984767 237:0.984767 238:0.984767 239:0.984767 240:0.984767 241:0.984767 242:0.984767 243:0.984767 244:0.984767 245:0.984767 246:0.984767 247:0.984767 248:0.984767 249:0.984767 250:0.984767
grad_clip_norm = 100.000000

augment_symmetries = 1 0 0
augment_rotation = vertical
augment_noise = 0.001000
augment_occlusion = none
augment_occlusion_ratio = 0.200000
augment_occlusion_num = 1
augment_scale_anisotropic = 1
augment_scale_min = 0.800000
augment_scale_max = 1.200000
augment_color = 0.800000

weight_decay = 0.001000
segloss_balance = none
class_w =
deform_fitting_mode = point2point
deform_fitting_power = 1.000000
deform_lr_factor = 0.100000
repulse_extent = 1.200000
batch_num = 8
val_batch_num = 8
max_epoch = 800
epoch_steps = 500
validation_size = 200
checkpoint_gap = 50
